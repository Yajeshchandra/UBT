{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "271a1ce0-6170-4c71-b78f-df667de6b908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-16 03:37:02.979833: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-16 03:37:03.696660: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing:  test\n",
      "Person  001\n",
      "Person  002\n",
      "Person  003\n",
      "Person  004\n",
      "Person  005\n",
      "Person  006\n",
      "Person  007\n",
      "Person  008\n",
      "Person  009\n",
      "Person  010\n",
      "Person  011\n",
      "Person  012\n",
      "Person  013\n",
      "Person  014\n",
      "Person  015\n",
      "Person  016\n",
      "Person  017\n",
      "Person  018\n",
      "Person  019\n",
      "Person  020\n",
      "Person  021\n",
      "Person  022\n",
      "Person  023\n",
      "Person  024\n",
      "Person  025\n",
      "Person  026\n",
      "Person  027\n",
      "Person  028\n",
      "Person  029\n",
      "Person  030\n",
      "Person  031\n",
      "Person  032\n",
      "Person  033\n",
      "Person  034\n",
      "Person  035\n",
      "Person  036\n",
      "Person  037\n",
      "Person  038\n",
      "Person  039\n",
      "Person  040\n",
      "Person  041\n",
      "Person  042\n",
      "Person  043\n",
      "Person  044\n",
      "Person  045\n",
      "Person  046\n",
      "Person  047\n",
      "Person  048\n",
      "Person  049\n",
      "Person  050\n",
      "Person  051\n",
      "Person  052\n",
      "Warning: dataset2/iris/test/052 has only 8 poses, expected at least 9\n",
      "Warning: dataset2/iris/test/052 has only 8 poses, expected at least 10\n",
      "Person  053\n",
      "Person  054\n",
      "Person  055\n",
      "Person  056\n",
      "Person  057\n",
      "Person  058\n",
      "Person  059\n",
      "Person  060\n",
      "Person  061\n",
      "Person  062\n",
      "Person  063\n",
      "Person  064\n",
      "Person  065\n",
      "Person  066\n",
      "Person  067\n",
      "Person  068\n",
      "Person  069\n",
      "Person  070\n",
      "Person  071\n",
      "Person  072\n",
      "Person  073\n",
      "Person  074\n",
      "Person  075\n",
      "Person  076\n",
      "Person  077\n",
      "Person  078\n",
      "Person  079\n",
      "Person  080\n",
      "Person  081\n",
      "Person  082\n",
      "Person  083\n",
      "Person  084\n",
      "Person  085\n",
      "Person  086\n",
      "Person  087\n",
      "Person  088\n",
      "Person  089\n",
      "Person  090\n",
      "Person  091\n",
      "Person  092\n",
      "Person  093\n",
      "Person  094\n",
      "Person  095\n",
      "Person  096\n",
      "Person  097\n",
      "Person  098\n",
      "Person  099\n",
      "Person  100\n",
      "Person  101\n",
      "Person  102\n",
      "Person  103\n",
      "Person  104\n",
      "Person  105\n",
      "Person  106\n",
      "Person  107\n",
      "Person  108\n",
      "Person  109\n",
      "Person  110\n",
      "Person  111\n",
      "Person  112\n",
      "Person  113\n",
      "Person  114\n",
      "Person  115\n",
      "Person  116\n",
      "Person  117\n",
      "Person  118\n",
      "Person  119\n",
      "Person  120\n",
      "Person  121\n",
      "Person  122\n",
      "Person  123\n",
      "Person  124\n",
      "Person  125\n",
      "Person  126\n",
      "Person  127\n",
      "Person  128\n",
      "Person  129\n",
      "Person  130\n",
      "Person  131\n",
      "Person  132\n",
      "Person  133\n",
      "Person  134\n",
      "Person  135\n",
      "Person  136\n",
      "Person  137\n",
      "Person  138\n",
      "Person  139\n",
      "Person  140\n",
      "Person  141\n",
      "Person  142\n",
      "Person  143\n",
      "Person  144\n",
      "Person  145\n",
      "Person  146\n",
      "Person  147\n",
      "Person  148\n",
      "Person  149\n",
      "Person  150\n",
      "Person  151\n",
      "Person  152\n",
      "Person  153\n",
      "Person  154\n",
      "Person  155\n",
      "Person  156\n",
      "Person  157\n",
      "Person  158\n",
      "Person  159\n",
      "Person  160\n",
      "Person  161\n",
      "Person  162\n",
      "Person  163\n",
      "Person  164\n",
      "Person  165\n",
      "Person  166\n",
      "Person  167\n",
      "Person  168\n",
      "Person  169\n",
      "Person  170\n",
      "Person  171\n",
      "Person  172\n",
      "Person  173\n",
      "Person  174\n",
      "Person  175\n",
      "Person  176\n",
      "Person  177\n",
      "Person  178\n",
      "Person  179\n",
      "Person  180\n",
      "Person  181\n",
      "Person  182\n",
      "Person  183\n",
      "Person  184\n",
      "Person  185\n",
      "Person  186\n",
      "Person  187\n",
      "Person  188\n",
      "Person  189\n",
      "Person  190\n",
      "Person  191\n",
      "Person  192\n",
      "Person  193\n",
      "Warning: dataset2/periocular/test/193 has only 9 poses, expected at least 10\n",
      "Person  194\n",
      "Person  195\n",
      "Person  196\n",
      "Person  197\n",
      "Person  198\n",
      "Person  199\n",
      "Person  200\n",
      "Person  201\n",
      "Person  202\n",
      "Person  203\n",
      "Person  204\n",
      "Person  205\n",
      "Person  206\n",
      "Person  207\n",
      "Person  208\n",
      "Person  209\n",
      "Person  210\n",
      "Person  211\n",
      "Warning: dataset2/periocular/test/211 has only 9 poses, expected at least 10\n",
      "Person  212\n",
      "Warning: dataset2/periocular/test/212 has only 4 poses, expected at least 5\n",
      "Warning: dataset2/periocular/test/212 has only 4 poses, expected at least 6\n",
      "Warning: dataset2/periocular/test/212 has only 4 poses, expected at least 7\n",
      "Warning: dataset2/periocular/test/212 has only 4 poses, expected at least 8\n",
      "Warning: dataset2/periocular/test/212 has only 4 poses, expected at least 9\n",
      "Warning: dataset2/periocular/test/212 has only 4 poses, expected at least 10\n",
      "Person  213\n",
      "Person  214\n",
      "Person  215\n",
      "Person  216\n",
      "Person  217\n",
      "Person  218\n",
      "Person  219\n",
      "Person  220\n",
      "Person  221\n",
      "Person  222\n",
      "Person  223\n",
      "Person  224\n",
      "Person  225\n",
      "Person  226\n",
      "Person  227\n",
      "Person  228\n",
      "Person  229\n",
      "Person  230\n",
      "Person  231\n",
      "Person  232\n",
      "Person  233\n",
      "Person  234\n",
      "Person  235\n",
      "Person  236\n",
      "Person  237\n",
      "Person  238\n",
      "Person  239\n",
      "Person  240\n",
      "Person  241\n",
      "Person  242\n",
      "Person  243\n",
      "Person  244\n",
      "Person  245\n",
      "Person  246\n",
      "Person  247\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch.nn.functional as F\n",
    "from dataset import BiometricDataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "class CNNEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, dropout_p=0.2):\n",
    "        super(CNNEmbedding, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=2, dilation=2)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=2, dilation=2)\n",
    "\n",
    "        self.res1 = nn.Conv2d(1, 64, kernel_size=1) if 1 != 64 else nn.Identity()\n",
    "        self.res2 = nn.Conv2d(64, 128, kernel_size=1) if 64 != 128 else nn.Identity()\n",
    "        self.res3 = nn.Conv2d(128, 256, kernel_size=1) if 128 != 256 else nn.Identity()\n",
    "        self.res4 = nn.Conv2d(256, 512, kernel_size=1) if 256 != 512 else nn.Identity()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(2)\n",
    "\n",
    "        # Depthwise separable convolution\n",
    "        self.depthwise_conv = nn.Conv2d(512, 512, kernel_size=3, padding=1, groups=512)\n",
    "        self.pointwise_conv = nn.Conv2d(512, 512, kernel_size=1)\n",
    "        self.depthwise_bn = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Squeeze-and-Excitation block\n",
    "        self.se_block = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(512, 512 // 16, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512 // 16, 512, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(512 * 2 * 2, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.ln = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.res1(x)\n",
    "        x = self.bn1(self.conv1(x))\n",
    "        x = F.relu(x + identity)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        identity = self.res2(x)\n",
    "        x = self.bn2(self.conv2(x))\n",
    "        x = F.relu(x + identity)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        identity = self.res3(x)\n",
    "        x = self.bn3(self.conv3(x))\n",
    "        x = F.relu(x + identity)\n",
    "        x = self.pool(x)\n",
    "\n",
    "        identity = self.res4(x)\n",
    "        x = self.bn4(self.conv4(x))\n",
    "        x = F.relu(x + identity)\n",
    "\n",
    "        # Depthwise separable convolution\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.pointwise_conv(x)\n",
    "        x = self.depthwise_bn(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Squeeze-and-Excitation\n",
    "        se = self.se_block(x)\n",
    "        x = x * se\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, nhead=8, num_layers=4, dropout_p=0.1, num_modalities=3, num_prompts=3):\n",
    "        super(FusionTransformer, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_modalities = num_modalities\n",
    "        \n",
    "        # Transformer encoder for global context\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=embedding_dim, \n",
    "                                       nhead=nhead, \n",
    "                                       dim_feedforward=512, \n",
    "                                       dropout=dropout_p, \n",
    "                                       batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Positional encodings\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, num_modalities + num_prompts, embedding_dim), requires_grad=True)\n",
    "        \n",
    "        # Layer norms for input and output\n",
    "        self.ln_in = nn.LayerNorm(embedding_dim)\n",
    "        self.ln_out = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        # MPT: Learnable prompt embeddings\n",
    "        self.num_prompts = num_prompts\n",
    "        self.prompt_emb = nn.Parameter(torch.randn(num_prompts, embedding_dim))\n",
    "        self.prompt_conv = nn.Conv1d(embedding_dim, embedding_dim, kernel_size=1)\n",
    "        self.prompt_relu = nn.ReLU()\n",
    "        \n",
    "        # Final projection\n",
    "        self.fc = nn.Linear(embedding_dim * (num_modalities + num_prompts), embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self._init_parameters()\n",
    "        \n",
    "    def _init_parameters(self):\n",
    "        # Initialize the prompt embeddings and positional encoder\n",
    "        nn.init.xavier_uniform_(self.prompt_emb)\n",
    "        nn.init.xavier_uniform_(self.pos_encoder)\n",
    "        \n",
    "    def forward(self, *embeddings):\n",
    "        batch_size = embeddings[0].size(0)\n",
    "        \n",
    "        # Stack embeddings for transformer input\n",
    "        modality_embeddings = torch.stack(embeddings, dim=1)  # [batch_size, num_modalities, embedding_dim]\n",
    "        \n",
    "        # Prepare prompt embeddings\n",
    "        prompt = self.prompt_emb.unsqueeze(0).repeat(batch_size, 1, 1)  # [batch_size, num_prompts, embedding_dim]\n",
    "        prompt = prompt.permute(0, 2, 1)  # [batch_size, embedding_dim, num_prompts]\n",
    "        prompt = self.prompt_conv(prompt)  # [batch_size, embedding_dim, num_prompts]\n",
    "        prompt = self.prompt_relu(prompt)\n",
    "        prompt = prompt.permute(0, 2, 1)  # [batch_size, num_prompts, embedding_dim]\n",
    "        \n",
    "        # Concatenate modality embeddings and prompts\n",
    "        x = torch.cat([modality_embeddings, prompt], dim=1)  # [batch_size, num_modalities + num_prompts, embedding_dim]\n",
    "        \n",
    "        # Apply layer norm and add positional encodings\n",
    "        x = self.ln_in(x) + self.pos_encoder\n",
    "        \n",
    "        # Apply transformer for global context\n",
    "        x = self.transformer(x)\n",
    "        \n",
    "        # Reshape and project to final embedding\n",
    "        x = x.reshape(batch_size, -1)  # [batch_size, (num_modalities + num_prompts) * embedding_dim]\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.ln_out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "# Complete Model\n",
    "class BiometricModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, num_modalities=3):\n",
    "        super(BiometricModel, self).__init__()\n",
    "\n",
    "        self.num_modalities = num_modalities\n",
    "\n",
    "        self.periocular_cnn = CNNEmbedding(embedding_dim)\n",
    "        self.forehead_cnn = CNNEmbedding(embedding_dim)\n",
    "        self.iris_cnn = CNNEmbedding(embedding_dim)\n",
    "\n",
    "        self.fusion_transformer = FusionTransformer(embedding_dim=256, num_modalities=num_modalities, num_prompts=3)\n",
    "        \n",
    "    def forward(self, periocular, forehead, iris):\n",
    "        periocular_emb = self.periocular_cnn(periocular)\n",
    "        forehead_emb = self.forehead_cnn(forehead)\n",
    "        iris_emb = self.iris_cnn(iris)\n",
    "        emb = [periocular_emb, forehead_emb, iris_emb]\n",
    "        fused_emb = self.fusion_transformer(*emb)\n",
    "        fused_emb = F.normalize(fused_emb, dim=1)\n",
    "        return fused_emb\n",
    "        \n",
    "\n",
    "def load_image(image_path, transform):\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    return transform(img)\n",
    "\n",
    "def create_embedding_dicts(data_root, model, device='mps'):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    modalities = ['periocular', 'forehead', 'iris']\n",
    "    splits = ['test']\n",
    "    train_dict = {}\n",
    "    test_dict = {}\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    for split in splits:\n",
    "        print(\"Doing: \", split)\n",
    "        target_dict = train_dict if split == 'train' else test_dict\n",
    "        # Get person IDs from directory\n",
    "        person_ids = [f\"{i:03d}\" for i in range(1, 248)]\n",
    "        for person_id in person_ids:\n",
    "            print(\"Person \", person_id)\n",
    "            target_dict[person_id] = []  # List to store 10 embeddings\n",
    "            # Iterate over poses (1 to 10)\n",
    "            for pose_idx in range(1, 11):\n",
    "                # Load images for all three modalities for this person and pose\n",
    "                images = []\n",
    "                for modality in modalities:\n",
    "                    img_path = Path(data_root) / modality / split / person_id\n",
    "                    pose_images = [f for f in sorted(os.listdir(img_path)) if f!='.DS_Store']\n",
    "                    if len(pose_images) < pose_idx:\n",
    "                        print(f\"Warning: {img_path} has only {len(pose_images)} poses, expected at least {pose_idx}\")\n",
    "                        break\n",
    "                    img_name = pose_images[pose_idx - 1]  # Select the pose_idx-th image\n",
    "                    img_path = img_path / img_name\n",
    "                    if not img_path.exists():\n",
    "                        print(f\"Warning: {img_path} does not exist\")\n",
    "                        break\n",
    "                    img = load_image(img_path, transform).to(device)\n",
    "                    images.append(img)\n",
    "                else:  # Only execute if all images are found\n",
    "                    # Pass three images to the model\n",
    "                    with torch.no_grad():\n",
    "                        embedding = model(\n",
    "                            images[0].unsqueeze(0),  # periocular (add batch dim)\n",
    "                            images[1].unsqueeze(0),  # forehead\n",
    "                            images[2].unsqueeze(0)   # iris\n",
    "                        ).squeeze(0).cpu()  # Remove batch dim\n",
    "                    target_dict[person_id].append(embedding)\n",
    "                if len(images) != 3:  # Skip if any image was missing\n",
    "                    continue\n",
    "\n",
    "    return train_dict, test_dict\n",
    "\n",
    "\n",
    "data_root = './dataset2'\n",
    "\n",
    "model = BiometricModel(embedding_dim=256)\n",
    "\n",
    "state_dict = torch.load('newmpt.pt')\n",
    "model.load_state_dict(state_dict)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "train_dict, test_dict = create_embedding_dicts(data_root, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a768cf-e49d-4a7a-b5f9-d4dfe9f0090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating rank-1 recognition performance...\n",
      "Skipping person 212 with only 4 embeddings\n",
      "Gallery size: 1230 embeddings\n",
      "Probe size: 1226 embeddings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 13/13 [00:00<00:00, 651.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank-1 recognition rate with gallery size 5: 0.7488 (74.88%) 918/1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_rank1(test_dict, gallery_size=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Evaluate rank-1 recognition accuracy using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        test_dict: Dictionary with person IDs as keys and lists of embeddings as values\n",
    "        gallery_size: Number of poses to use as gallery (remaining used as probe)\n",
    "        device: Device to perform calculations on\n",
    "    \n",
    "    Returns:\n",
    "        Rank-1 recognition rate\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Convert embeddings to tensors and prepare gallery and probe sets\n",
    "    gallery_embeddings = []\n",
    "    gallery_labels = []\n",
    "    probe_embeddings = []\n",
    "    probe_labels = []\n",
    "    \n",
    "    for person_id, embeddings in test_dict.items():\n",
    "        if len(embeddings) < gallery_size + 1:\n",
    "            print(f\"Skipping person {person_id} with only {len(embeddings)} embeddings\")\n",
    "            continue\n",
    "            \n",
    "        # Use first gallery_size embeddings as gallery\n",
    "        for i in range(gallery_size):\n",
    "            gallery_embeddings.append(embeddings[i])\n",
    "            gallery_labels.append(person_id)\n",
    "            \n",
    "        # Use remaining embeddings as probe\n",
    "        for i in range(gallery_size, len(embeddings)):\n",
    "            probe_embeddings.append(embeddings[i])\n",
    "            probe_labels.append(person_id)\n",
    "    \n",
    "    gallery_tensor = torch.stack(gallery_embeddings).to(device)\n",
    "    probe_tensor = torch.stack(probe_embeddings).to(device)\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    gallery_tensor = F.normalize(gallery_tensor, p=2, dim=1)\n",
    "    probe_tensor = F.normalize(probe_tensor, p=2, dim=1)\n",
    "    \n",
    "    print(f\"Gallery size: {len(gallery_tensor)} embeddings\")\n",
    "    print(f\"Probe size: {len(probe_tensor)} embeddings\")\n",
    "    \n",
    "    # Calculate similarities in batches to avoid OOM\n",
    "    batch_size = 100\n",
    "    correct = 0\n",
    "    total = len(probe_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(probe_tensor), batch_size)):\n",
    "            batch_end = min(i + batch_size, len(probe_tensor))\n",
    "            batch_probe = probe_tensor[i:batch_end]\n",
    "            \n",
    "            # Calculate cosine similarity between probe and gallery\n",
    "            similarities = torch.mm(batch_probe, gallery_tensor.t())\n",
    "            \n",
    "            # Get the indices of the highest similarities\n",
    "            _, indices = torch.max(similarities, dim=1)\n",
    "            \n",
    "            # Check if the prediction is correct\n",
    "            for j in range(len(batch_probe)):\n",
    "                probe_person = probe_labels[i + j]\n",
    "                predicted_person = gallery_labels[indices[j].item()]\n",
    "                \n",
    "                if probe_person == predicted_person:\n",
    "                    correct += 1\n",
    "    \n",
    "    # Calculate rank-1 recognition rate\n",
    "    rank1_rate = correct / total if total > 0 else 0\n",
    "    return rank1_rate, correct, total\n",
    "\n",
    "# Add this code after loading the model and creating embeddings\n",
    "print(\"Evaluating rank-1 recognition performance...\")\n",
    "\n",
    "# Test with different gallery sizes\n",
    "for gallery_size in [5]:\n",
    "    rank1_rate, c, t = evaluate_rank1(test_dict, gallery_size=gallery_size, device=device)\n",
    "    print(f\"Rank-1 recognition rate with gallery size {gallery_size}: {rank1_rate:.4f} ({rank1_rate*100:.2f}%) {c}/{t}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
