{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa4821b2-5314-4f0d-b886-2134be3559a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ loaded weights from model_latest_mpt_best.pt\n",
      "✓ saved 15 heat-maps to ‘heatmaps’\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os, sys, torch, matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "\n",
    "# ════════════════════════════════════\n",
    "# 1. CONFIG –- EDIT THESE THREE LINES\n",
    "# ════════════════════════════════════\n",
    "DATA_ROOT  = \"/home/teaching/Desktop/dl123/dataset2\"      # dataset2 root\n",
    "CKPT       = \"model_latest_mpt_best.pt\"                   # checkpoint (.pt) – \"\" to skip\n",
    "OUT_DIR    = \"heatmaps\"                                   # where PNGs will be saved\n",
    "NUM_SAMPLES = 5                                          # triplets you want to visualise\n",
    "BATCH_SIZE  = 32\n",
    "SPLIT       = \"test\"                                      # train | val | test\n",
    "# ════════════════════════════════════\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  Model definition  (your code verbatim)\n",
    "# ------------------------------------------------------------\n",
    "class CNNEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, dropout_p=0.3):\n",
    "        super(CNNEmbedding, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, 1, 1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, 3, 1, 1)\n",
    "\n",
    "        self.res1 = nn.Conv2d(1, 64, 1)   if 1   != 64  else nn.Identity()\n",
    "        self.res2 = nn.Conv2d(64, 128, 1) if 64  != 128 else nn.Identity()\n",
    "        self.res3 = nn.Conv2d(128, 256, 1)if 128 != 256 else nn.Identity()\n",
    "        self.res4 = nn.Conv2d(256, 512, 1)if 256 != 512 else nn.Identity()\n",
    "\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d(4)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.fc  = nn.Linear(512*4*4, embedding_dim)\n",
    "        self.ln  = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.res1(x); x = F.relu(self.bn1(self.conv1(x))+identity); x=self.pool(x)\n",
    "        identity = self.res2(x); x = F.relu(self.bn2(self.conv2(x))+identity); x=self.pool(x)\n",
    "        identity = self.res3(x); x = F.relu(self.bn3(self.conv3(x))+identity); x=self.pool(x)\n",
    "        identity = self.res4(x); x = F.relu(self.bn4(self.conv4(x))+identity)               # conv4 feat-map stored in hook\n",
    "        x = self.adaptive_pool(x).flatten(1)\n",
    "        x = self.ln(self.fc(self.dropout(x)))\n",
    "        return x\n",
    "\n",
    "class FusionTransformer(nn.Module):\n",
    "    def __init__(self, embedding_dim=256, nhead=8, num_layers=4,\n",
    "                 dropout_p=0.1, num_modalities=3, num_prompts=3):\n",
    "        super().__init__()\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(embedding_dim, nhead,\n",
    "                                       dim_feedforward=512,\n",
    "                                       dropout=dropout_p,\n",
    "                                       batch_first=True),\n",
    "            num_layers=num_layers)\n",
    "        self.pos = nn.Parameter(torch.zeros(1, num_modalities+num_prompts, embedding_dim))\n",
    "        self.ln_in  = nn.LayerNorm(embedding_dim)\n",
    "        self.ln_out = nn.LayerNorm(embedding_dim)\n",
    "        self.prompt = nn.Parameter(torch.randn(num_prompts, embedding_dim))\n",
    "        self.p_conv = nn.Conv1d(embedding_dim, embedding_dim, 1)\n",
    "        self.fc     = nn.Linear((num_modalities+num_prompts)*embedding_dim, embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self,*E):\n",
    "        b = E[0].size(0)\n",
    "        mod = torch.stack(E,1)                      # (B,3,256)\n",
    "        p   = self.prompt.unsqueeze(0).expand(b,-1,-1).permute(0,2,1) # (B,256,3)\n",
    "        p   = self.p_conv(p).permute(0,2,1)        # (B,3,256)\n",
    "        x   = torch.cat([mod, p],1)                # (B,6,256)\n",
    "        x   = self.ln_in(x)+self.pos\n",
    "        x   = self.transformer(x)\n",
    "        x   = self.fc(self.dropout(x.flatten(1)))\n",
    "        return self.ln_out(x)\n",
    "\n",
    "class BiometricModel(nn.Module):\n",
    "    def __init__(self, embedding_dim=256):\n",
    "        super().__init__()\n",
    "        self.periocular_cnn = CNNEmbedding(embedding_dim)\n",
    "        self.forehead_cnn   = CNNEmbedding(embedding_dim)\n",
    "        self.iris_cnn       = CNNEmbedding(embedding_dim)\n",
    "        self.fusion_transformer = FusionTransformer(embedding_dim, num_modalities=3)\n",
    "\n",
    "    def forward(self, peri, fore, iris):\n",
    "        e1 = self.periocular_cnn(peri)\n",
    "        e2 = self.forehead_cnn(fore)\n",
    "        e3 = self.iris_cnn(iris)\n",
    "        out = self.fusion_transformer(e1,e2,e3)\n",
    "        return F.normalize(out, dim=1)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  Device, model, checkpoint\n",
    "# ------------------------------------------------------------\n",
    "DEVICE = (torch.device(\"cuda\") if torch.cuda.is_available()\n",
    "          else torch.device(\"mps\") if torch.backends.mps.is_available()\n",
    "          else torch.device(\"cpu\"))\n",
    "\n",
    "model = BiometricModel(embedding_dim=256).to(DEVICE).eval()\n",
    "if CKPT and Path(CKPT).is_file():\n",
    "    model.load_state_dict(torch.load(CKPT, map_location=DEVICE), strict=False)\n",
    "    print(f\"✔ loaded weights from {CKPT}\")\n",
    "else:\n",
    "    print(\"⚠ running with random weights (checkpoint not found / skipped)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  Attach Grad-CAM hooks to *.conv4*\n",
    "# ------------------------------------------------------------\n",
    "feature_maps, gradients = {}, {}\n",
    "def fwd(lbl): return lambda _,__,o: feature_maps.__setitem__(lbl,o.detach())\n",
    "def bwd(lbl): return lambda _,__,go: gradients.__setitem__(lbl,go[0].detach())\n",
    "\n",
    "for lbl, branch in [(\"periocular\", model.periocular_cnn),\n",
    "                    (\"forehead\",   model.forehead_cnn),\n",
    "                    (\"iris\",       model.iris_cnn)]:\n",
    "    if not hasattr(branch,\"conv4\"):\n",
    "        print(f\"❌ {lbl}_cnn has no conv4 – abort\"); sys.exit(1)\n",
    "    branch.conv4.register_forward_hook(fwd(lbl))\n",
    "    branch.conv4.register_full_backward_hook(bwd(lbl))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 5.  Pre-processing\n",
    "# ------------------------------------------------------------\n",
    "tx = transforms.Compose([\n",
    "    transforms.Resize((128,128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "denorm = lambda t: ((t*0.5+0.5).clamp(0,1)*255).byte().cpu().numpy()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 6.  Iterate dataset, make heat-maps\n",
    "# ------------------------------------------------------------\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "modalities = [\"periocular\",\"forehead\",\"iris\"]\n",
    "pids       = [f\"{i:03d}\" for i in range(1,248)]\n",
    "\n",
    "batch = {m: [] for m in modalities}; saved = 0\n",
    "for pid in pids:\n",
    "    for pose in range(1,11):\n",
    "        imgs=[]; ok=True\n",
    "        for m in modalities:\n",
    "            folder = Path(DATA_ROOT)/m/SPLIT/pid\n",
    "            if not folder.is_dir(): ok=False; break\n",
    "            files  = sorted(f for f in os.listdir(folder) if not f.startswith('.'))\n",
    "            if pose>len(files): ok=False; break\n",
    "            imgs.append(tx(Image.open(folder/files[pose-1]).convert(\"L\")))\n",
    "        if not ok: continue\n",
    "        for m,t in zip(modalities,imgs): batch[m].append(t)\n",
    "\n",
    "        last = pid==pids[-1] and pose==10\n",
    "        if len(batch[\"periocular\"])==BATCH_SIZE or last:\n",
    "            peri=torch.stack(batch[\"periocular\"]).to(DEVICE)\n",
    "            fore=torch.stack(batch[\"forehead\"]).to(DEVICE)\n",
    "            iris=torch.stack(batch[\"iris\"]).to(DEVICE)\n",
    "\n",
    "            feature_maps.clear(); gradients.clear()\n",
    "            model.zero_grad()\n",
    "            model(peri,fore,iris).norm(dim=1).sum().backward()\n",
    "\n",
    "            for i in range(peri.size(0)):\n",
    "                if saved>=NUM_SAMPLES: break\n",
    "                for m,src in zip(modalities,[peri,fore,iris]):\n",
    "                    fmap=feature_maps[m][i]; grad=gradients[m][i]\n",
    "                    w=grad.mean((1,2),keepdim=True)\n",
    "                    cam=(w*fmap).sum(0).relu()\n",
    "                    cam = F.interpolate(\n",
    "                                cam[None, None],               # add batch & channel dims\n",
    "                                size=(128, 128),               # target H×W\n",
    "                                mode=\"bilinear\",\n",
    "                                align_corners=False\n",
    "                              )[0, 0]\n",
    "\n",
    "                    cam=(cam-cam.min())/(cam.max()+1e-8); cam=cam.cpu().numpy()\n",
    "                    plt.figure(figsize=(3,3))\n",
    "                    plt.imshow(denorm(src[i]).squeeze(),cmap=\"gray\")\n",
    "                    plt.imshow(cam,cmap=\"jet\",alpha=0.5); plt.axis(\"off\")\n",
    "                    plt.tight_layout(pad=0)\n",
    "                    plt.savefig(f\"{OUT_DIR}/{m}_heatmap_{saved+1}.png\",\n",
    "                                dpi=140,bbox_inches=\"tight\")\n",
    "                    plt.close()\n",
    "                saved+=1\n",
    "                if saved>=NUM_SAMPLES: break\n",
    "            batch = {m: [] for m in modalities}\n",
    "        if saved>=NUM_SAMPLES: break\n",
    "    if saved>=NUM_SAMPLES: break\n",
    "\n",
    "print(f\"✓ saved {saved*3} heat-maps to ‘{OUT_DIR}’\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
