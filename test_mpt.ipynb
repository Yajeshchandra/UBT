{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b6ec4c9",
   "metadata": {},
   "source": [
    "# Biometric Fusion with MPT Testing\n",
    "\n",
    "This notebook implements the testing and evaluation process for the Multi-Modal Biometric Fusion model with Modified Prompt Tuning (MPT).\n",
    "\n",
    "## Overview\n",
    "\n",
    "The testing includes:\n",
    "1. Rank-1 recognition accuracy \n",
    "2. ROC curve and EER calculation\n",
    "3. Modality-specific performance analysis\n",
    "4. Embedding visualization using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191c8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from model.model_mpt import BiometricModel\n",
    "from model.dataset import BiometricDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8b2ff",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Set up parameters for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3aed03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Testing parameters\n",
    "params = {\n",
    "    'data_path': './dataset2',                   # Path to dataset\n",
    "    'model_path': './checkpoints/model_latest_mpt_best.pt',  # Path to trained model\n",
    "    'embedding_dim': 256,                         # Dimension of embeddings\n",
    "    'batch_size': 32,                             # Batch size for testing\n",
    "    'output_dir': './results',                    # Directory to save results\n",
    "    'gallery_sizes': [1, 3, 5, 7, 9],             # Gallery sizes to test\n",
    "    'visualize_persons': 30                       # Number of persons for visualization\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(params['output_dir'], exist_ok=True)\n",
    "\n",
    "# Define transformation for testing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5339c6c2",
   "metadata": {},
   "source": [
    "## Load Model\n",
    "\n",
    "Load the trained model for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fe300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = BiometricModel(embedding_dim=params['embedding_dim'])\n",
    "model.load_state_dict(torch.load(params['model_path'], map_location=device))\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"Loaded model from {params['model_path']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d78a5c9",
   "metadata": {},
   "source": [
    "## Embedding Extraction\n",
    "\n",
    "Create functions to extract embeddings for all test images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9138223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform):\n",
    "    \"\"\"Load and transform an image\"\"\"\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    return transform(img)\n",
    "\n",
    "def create_embedding_dicts(data_root, model, device=device):\n",
    "    \"\"\"\n",
    "    Create dictionaries of embeddings for all test images\n",
    "    Returns a dictionary with person IDs as keys and lists of embeddings as values\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "\n",
    "    modalities = ['periocular', 'forehead', 'iris']\n",
    "    split = 'test'\n",
    "    test_dict = {}\n",
    "\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Extracting embeddings for {split} set...\")\n",
    "    # Get person IDs from directory\n",
    "    person_ids = [f\"{i:03d}\" for i in range(1, 248)]\n",
    "    \n",
    "    for person_id in tqdm(person_ids):\n",
    "        test_dict[person_id] = []  # List to store embeddings\n",
    "        # Iterate over poses (1 to 10)\n",
    "        for pose_idx in range(1, 11):\n",
    "            # Load images for all three modalities for this person and pose\n",
    "            images = []\n",
    "            for modality in modalities:\n",
    "                img_path = Path(data_root) / modality / split / person_id\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "                    \n",
    "                pose_images = [f for f in sorted(os.listdir(img_path)) if f!='.DS_Store']\n",
    "                if len(pose_images) < pose_idx:\n",
    "                    break\n",
    "                    \n",
    "                img_name = pose_images[pose_idx - 1]  # Select the pose_idx-th image\n",
    "                img_path = img_path / img_name\n",
    "                if not img_path.exists():\n",
    "                    break\n",
    "                    \n",
    "                img = load_image(img_path, transform).to(device)\n",
    "                images.append(img)\n",
    "                \n",
    "            # Only execute if all images are found\n",
    "            if len(images) == 3:\n",
    "                # Pass three images to the model\n",
    "                with torch.no_grad():\n",
    "                    embedding = model(\n",
    "                        images[0].unsqueeze(0),  # periocular (add batch dim)\n",
    "                        images[1].unsqueeze(0),  # forehead\n",
    "                        images[2].unsqueeze(0)   # iris\n",
    "                    ).squeeze(0).cpu()  # Remove batch dim\n",
    "                test_dict[person_id].append(embedding)\n",
    "\n",
    "    # Remove persons with no embeddings\n",
    "    test_dict = {k: v for k, v in test_dict.items() if v}\n",
    "    print(f\"Created embeddings for {len(test_dict)} persons\")\n",
    "    \n",
    "    return test_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985d569d",
   "metadata": {},
   "source": [
    "## Rank-1 Recognition Evaluation\n",
    "\n",
    "Evaluate Rank-1 recognition accuracy with different gallery sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020acbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rank1(test_dict, gallery_size=5, device=device):\n",
    "    \"\"\"\n",
    "    Evaluate rank-1 recognition accuracy using cosine similarity.\n",
    "    \n",
    "    Args:\n",
    "        test_dict: Dictionary with person IDs as keys and lists of embeddings as values\n",
    "        gallery_size: Number of poses to use as gallery (remaining used as probe)\n",
    "        device: Device to perform calculations on\n",
    "    \n",
    "    Returns:\n",
    "        Rank-1 recognition rate, number of correct identifications, total number of probes\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Convert embeddings to tensors and prepare gallery and probe sets\n",
    "    gallery_embeddings = []\n",
    "    gallery_labels = []\n",
    "    probe_embeddings = []\n",
    "    probe_labels = []\n",
    "    \n",
    "    for person_id, embeddings in test_dict.items():\n",
    "        if len(embeddings) < gallery_size + 1:\n",
    "            continue\n",
    "            \n",
    "        # Use first gallery_size embeddings as gallery\n",
    "        for i in range(gallery_size):\n",
    "            gallery_embeddings.append(embeddings[i])\n",
    "            gallery_labels.append(person_id)\n",
    "            \n",
    "        # Use remaining embeddings as probe\n",
    "        for i in range(gallery_size, len(embeddings)):\n",
    "            probe_embeddings.append(embeddings[i])\n",
    "            probe_labels.append(person_id)\n",
    "    \n",
    "    gallery_tensor = torch.stack(gallery_embeddings).to(device)\n",
    "    probe_tensor = torch.stack(probe_embeddings).to(device)\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    gallery_tensor = F.normalize(gallery_tensor, p=2, dim=1)\n",
    "    probe_tensor = F.normalize(probe_tensor, p=2, dim=1)\n",
    "    \n",
    "    print(f\"Gallery size: {len(gallery_tensor)} embeddings\")\n",
    "    print(f\"Probe size: {len(probe_tensor)} embeddings\")\n",
    "    \n",
    "    # Calculate similarities in batches to avoid OOM\n",
    "    batch_size = 100\n",
    "    correct = 0\n",
    "    total = len(probe_tensor)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(probe_tensor), batch_size):\n",
    "            batch_end = min(i + batch_size, len(probe_tensor))\n",
    "            batch_probe = probe_tensor[i:batch_end]\n",
    "            \n",
    "            # Calculate cosine similarity between probe and gallery\n",
    "            similarities = torch.mm(batch_probe, gallery_tensor.t())\n",
    "            \n",
    "            # Get the indices of the highest similarities\n",
    "            _, indices = torch.max(similarities, dim=1)\n",
    "            \n",
    "            # Check if the prediction is correct\n",
    "            for j in range(len(batch_probe)):\n",
    "                probe_person = probe_labels[i + j]\n",
    "                predicted_person = gallery_labels[indices[j].item()]\n",
    "                \n",
    "                if probe_person == predicted_person:\n",
    "                    correct += 1\n",
    "    \n",
    "    # Calculate rank-1 recognition rate\n",
    "    rank1_rate = correct / total if total > 0 else 0\n",
    "    return rank1_rate, correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416c9fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract embeddings for all test images\n",
    "test_dict = create_embedding_dicts(params['data_path'], model, device)\n",
    "\n",
    "# Evaluate rank-1 recognition for different gallery sizes\n",
    "print(\"Evaluating rank-1 recognition performance...\")\n",
    "results = []\n",
    "\n",
    "for gallery_size in params['gallery_sizes']:\n",
    "    rank1_rate, correct, total = evaluate_rank1(test_dict, gallery_size=gallery_size, device=device)\n",
    "    results.append({\n",
    "        'gallery_size': gallery_size,\n",
    "        'rank1_rate': rank1_rate,\n",
    "        'correct': correct,\n",
    "        'total': total\n",
    "    })\n",
    "    print(f\"Rank-1 recognition rate with gallery size {gallery_size}: {rank1_rate:.4f} ({rank1_rate*100:.2f}%) {correct}/{total}\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([r['gallery_size'] for r in results], [r['rank1_rate']*100 for r in results], 'o-', linewidth=2)\n",
    "plt.xlabel('Gallery Size')\n",
    "plt.ylabel('Rank-1 Recognition Rate (%)')\n",
    "plt.title('Rank-1 Recognition Rate vs. Gallery Size')\n",
    "plt.grid(True)\n",
    "plt.xticks([r['gallery_size'] for r in results])\n",
    "plt.savefig(f\"{params['output_dir']}/rank1_vs_gallery_size.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Save results to file\n",
    "with open(f\"{params['output_dir']}/rank1_results.txt\", 'w') as f:\n",
    "    f.write(\"Gallery Size | Rank-1 Rate | Correct/Total\\n\")\n",
    "    f.write(\"-------------|-------------|-------------\\n\")\n",
    "    for r in results:\n",
    "        f.write(f\"{r['gallery_size']:12d} | {r['rank1_rate']*100:10.2f}% | {r['correct']}/{r['total']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded38142",
   "metadata": {},
   "source": [
    "## Verification Performance (ROC Curve and EER)\n",
    "\n",
    "Evaluate the verification performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955495e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_verification_metrics(test_dict, gallery_size=5, device=device):\n",
    "    \"\"\"Compute verification metrics (ROC curve, EER)\"\"\"\n",
    "    # Prepare gallery and probe sets\n",
    "    gallery_embeddings = []\n",
    "    gallery_labels = []\n",
    "    probe_embeddings = []\n",
    "    probe_labels = []\n",
    "    \n",
    "    for person_id, embeddings in test_dict.items():\n",
    "        if len(embeddings) < gallery_size + 1:\n",
    "            continue\n",
    "            \n",
    "        # Use first gallery_size embeddings as gallery\n",
    "        for i in range(gallery_size):\n",
    "            gallery_embeddings.append(embeddings[i])\n",
    "            gallery_labels.append(person_id)\n",
    "            \n",
    "        # Use remaining embeddings as probe\n",
    "        for i in range(gallery_size, len(embeddings)):\n",
    "            probe_embeddings.append(embeddings[i])\n",
    "            probe_labels.append(person_id)\n",
    "    \n",
    "    gallery_tensor = torch.stack(gallery_embeddings).to(device)\n",
    "    probe_tensor = torch.stack(probe_embeddings).to(device)\n",
    "    \n",
    "    # Normalize embeddings for cosine similarity\n",
    "    gallery_tensor = F.normalize(gallery_tensor, p=2, dim=1)\n",
    "    probe_tensor = F.normalize(probe_tensor, p=2, dim=1)\n",
    "    \n",
    "    # Compute all pairwise similarities\n",
    "    similarities = torch.mm(probe_tensor, gallery_tensor.t()).cpu().numpy()\n",
    "    \n",
    "    # Create ground truth labels for all pairs\n",
    "    y_true = []\n",
    "    y_scores = []\n",
    "    \n",
    "    for i, probe_label in enumerate(probe_labels):\n",
    "        for j, gallery_label in enumerate(gallery_labels):\n",
    "            # Ground truth: 1 if same person, 0 if different\n",
    "            y_true.append(1 if probe_label == gallery_label else 0)\n",
    "            y_scores.append(similarities[i, j])\n",
    "    \n",
    "    # Compute ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Compute Equal Error Rate (EER)\n",
    "    fnr = 1 - tpr\n",
    "    eer_idx = np.argmin(np.abs(fnr - fpr))\n",
    "    eer = (fpr[eer_idx] + fnr[eer_idx]) / 2\n",
    "    eer_threshold = thresholds[eer_idx]\n",
    "    \n",
    "    # Find thresholds for specific FARs (False Accept Rates)\n",
    "    far_thresholds = {}\n",
    "    for target_far in [0.001, 0.01, 0.1]:\n",
    "        idx = np.argmin(np.abs(fpr - target_far))\n",
    "        far_thresholds[target_far] = {\n",
    "            'threshold': thresholds[idx],\n",
    "            'far': fpr[idx],\n",
    "            'frr': 1 - tpr[idx],\n",
    "            'tar': tpr[idx]  # True Accept Rate\n",
    "        }\n",
    "    \n",
    "    # Separate genuine and impostor scores for distribution plotting\n",
    "    genuine_scores = [y_scores[i] for i in range(len(y_true)) if y_true[i] == 1]\n",
    "    impostor_scores = [y_scores[i] for i in range(len(y_true)) if y_true[i] == 0]\n",
    "    \n",
    "    return {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'thresholds': thresholds,\n",
    "        'roc_auc': roc_auc,\n",
    "        'eer': eer,\n",
    "        'eer_threshold': eer_threshold,\n",
    "        'genuine_scores': genuine_scores,\n",
    "        'impostor_scores': impostor_scores,\n",
    "        'far_thresholds': far_thresholds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4bb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute verification metrics\n",
    "print(\"Computing verification metrics...\")\n",
    "gallery_size = 5  # Use a standard gallery size for verification metrics\n",
    "verification_metrics = compute_verification_metrics(test_dict, gallery_size=gallery_size, device=device)\n",
    "\n",
    "# Print results\n",
    "print(f\"ROC AUC: {verification_metrics['roc_auc']:.4f}\")\n",
    "print(f\"EER: {verification_metrics['eer']:.4f}\")\n",
    "print(f\"EER Threshold: {verification_metrics['eer_threshold']:.4f}\")\n",
    "\n",
    "# Print FARs and FRRs at specific thresholds\n",
    "print(\"\\nPerformance at specific FARs:\")\n",
    "for far, data in verification_metrics['far_thresholds'].items():\n",
    "    print(f\"FAR = {far:.4f}: Threshold = {data['threshold']:.4f}, FRR = {data['frr']:.4f}, TAR = {data['tar']:.4f}\")\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(\n",
    "    verification_metrics['fpr'], \n",
    "    verification_metrics['tpr'], \n",
    "    color='darkorange',\n",
    "    lw=2, \n",
    "    label=f'ROC curve (area = {verification_metrics[\"roc_auc\"]:.4f})'\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{params['output_dir']}/roc_curve.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Plot score distributions\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.hist(\n",
    "    verification_metrics['genuine_scores'], \n",
    "    bins=50, \n",
    "    alpha=0.5, \n",
    "    color='green', \n",
    "    label='Genuine Pairs'\n",
    ")\n",
    "plt.hist(\n",
    "    verification_metrics['impostor_scores'], \n",
    "    bins=50, \n",
    "    alpha=0.5, \n",
    "    color='red', \n",
    "    label='Impostor Pairs'\n",
    ")\n",
    "plt.axvline(x=verification_metrics['eer_threshold'], color='black', linestyle='--', \n",
    "            label=f'EER Threshold = {verification_metrics[\"eer_threshold\"]:.4f}')\n",
    "plt.xlabel('Similarity Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Similarity Scores')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(f\"{params['output_dir']}/score_distribution.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Save results to file\n",
    "with open(f\"{params['output_dir']}/verification_metrics.txt\", 'w') as f:\n",
    "    f.write(f\"ROC AUC: {verification_metrics['roc_auc']:.4f}\\n\")\n",
    "    f.write(f\"EER: {verification_metrics['eer']:.4f}\\n\")\n",
    "    f.write(f\"EER Threshold: {verification_metrics['eer_threshold']:.4f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Performance at specific FARs:\\n\")\n",
    "    for far, data in verification_metrics['far_thresholds'].items():\n",
    "        f.write(f\"FAR = {far:.4f}: Threshold = {data['threshold']:.4f}, FRR = {data['frr']:.4f}, TAR = {data['tar']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a37d75f",
   "metadata": {},
   "source": [
    "## Individual Modality Analysis\n",
    "\n",
    "Analyze the performance of individual modalities versus fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_modalities(test_dict, model, device=device):\n",
    "    \"\"\"\n",
    "    Analyze the performance of individual modalities vs. fusion\n",
    "    \n",
    "    Args:\n",
    "        test_dict: Dictionary with person IDs as keys and lists of embeddings as values\n",
    "        model: BiometricModel instance\n",
    "        device: Device to run computations on\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with rank-1 recognition rates for each modality\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    # Modalities to test\n",
    "    modalities = ['periocular', 'forehead', 'iris', 'fusion']\n",
    "    \n",
    "    # Dictionary to store embeddings for each modality\n",
    "    modality_dict = {modality: {} for modality in modalities}\n",
    "    \n",
    "    # Extract embeddings for each modality\n",
    "    print(\"Extracting embeddings for individual modalities...\")\n",
    "    for person_id, embeddings in tqdm(test_dict.items()):\n",
    "        for modality in modalities:\n",
    "            modality_dict[modality][person_id] = []\n",
    "    \n",
    "    # Process each person and pose\n",
    "    for person_id in tqdm(test_dict.keys()):\n",
    "        # For each pose (1 to 10)\n",
    "        for pose_idx in range(1, 11):\n",
    "            # Load images for all three modalities\n",
    "            images = []\n",
    "            for mod_idx, modality in enumerate(['periocular', 'forehead', 'iris']):\n",
    "                img_path = Path(params['data_path']) / modality / 'test' / person_id\n",
    "                if not img_path.exists():\n",
    "                    continue\n",
    "                    \n",
    "                pose_images = [f for f in sorted(os.listdir(img_path)) if f!='.DS_Store']\n",
    "                if len(pose_images) < pose_idx:\n",
    "                    break\n",
    "                    \n",
    "                img_name = pose_images[pose_idx - 1]\n",
    "                img_path = img_path / img_name\n",
    "                if not img_path.exists():\n",
    "                    break\n",
    "                    \n",
    "                img = load_image(img_path, transform).to(device)\n",
    "                images.append(img)\n",
    "                \n",
    "            # Only proceed if all images are found\n",
    "            if len(images) == 3:\n",
    "                # Process each modality separately\n",
    "                with torch.no_grad():\n",
    "                    # Periocular\n",
    "                    periocular_emb = model.periocular_cnn(images[0].unsqueeze(0))\n",
    "                    periocular_emb = F.normalize(periocular_emb, dim=1).cpu().squeeze(0)\n",
    "                    modality_dict['periocular'][person_id].append(periocular_emb)\n",
    "                    \n",
    "                    # Forehead\n",
    "                    forehead_emb = model.forehead_cnn(images[1].unsqueeze(0))\n",
    "                    forehead_emb = F.normalize(forehead_emb, dim=1).cpu().squeeze(0)\n",
    "                    modality_dict['forehead'][person_id].append(forehead_emb)\n",
    "                    \n",
    "                    # Iris\n",
    "                    iris_emb = model.iris_cnn(images[2].unsqueeze(0))\n",
    "                    iris_emb = F.normalize(iris_emb, dim=1).cpu().squeeze(0)\n",
    "                    modality_dict['iris'][person_id].append(iris_emb)\n",
    "                    \n",
    "                    # Fusion (all modalities combined)\n",
    "                    fusion_emb = model(\n",
    "                        images[0].unsqueeze(0),\n",
    "                        images[1].unsqueeze(0),\n",
    "                        images[2].unsqueeze(0)\n",
    "                    ).cpu().squeeze(0)\n",
    "                    modality_dict['fusion'][person_id].append(fusion_emb)\n",
    "    \n",
    "    # Evaluate rank-1 recognition for each modality\n",
    "    results = {}\n",
    "    gallery_size = 5\n",
    "    \n",
    "    print(\"Evaluating rank-1 recognition for each modality...\")\n",
    "    for modality in modalities:\n",
    "        # Filter out persons with insufficient samples\n",
    "        filtered_dict = {k: v for k, v in modality_dict[modality].items() if len(v) >= gallery_size + 1}\n",
    "        if not filtered_dict:\n",
    "            print(f\"Warning: No valid samples for {modality}\")\n",
    "            results[modality] = 0\n",
    "            continue\n",
    "            \n",
    "        rank1_rate, correct, total = evaluate_rank1(filtered_dict, gallery_size=gallery_size, device=device)\n",
    "        results[modality] = {\n",
    "            'rank1_rate': rank1_rate,\n",
    "            'correct': correct,\n",
    "            'total': total\n",
    "        }\n",
    "        print(f\"{modality.capitalize()} Rank-1: {rank1_rate:.4f} ({rank1_rate*100:.2f}%) {correct}/{total}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654fc29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze individual modalities vs. fusion\n",
    "print(\"Analyzing individual modalities vs. fusion...\")\n",
    "modality_results = analyze_modalities(test_dict, model, device)\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(10, 6))\n",
    "modalities = ['periocular', 'forehead', 'iris', 'fusion']\n",
    "rates = [modality_results[m]['rank1_rate']*100 for m in modalities]\n",
    "colors = ['blue', 'green', 'red', 'purple']\n",
    "\n",
    "plt.bar(modalities, rates, color=colors)\n",
    "plt.ylabel('Rank-1 Recognition Rate (%)')\n",
    "plt.title('Performance Comparison: Individual Modalities vs. Fusion')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add text labels on top of each bar\n",
    "for i, v in enumerate(rates):\n",
    "    plt.text(i, v + 1, f\"{v:.1f}%\", ha='center')\n",
    "\n",
    "plt.savefig(f\"{params['output_dir']}/modality_comparison.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Save results to file\n",
    "with open(f\"{params['output_dir']}/modality_results.txt\", 'w') as f:\n",
    "    f.write(\"Modality | Rank-1 Rate | Correct/Total\\n\")\n",
    "    f.write(\"---------|-------------|-------------\\n\")\n",
    "    for modality in modalities:\n",
    "        r = modality_results[modality]\n",
    "        f.write(f\"{modality.capitalize():8s} | {r['rank1_rate']*100:10.2f}% | {r['correct']}/{r['total']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4587225",
   "metadata": {},
   "source": [
    "## Embedding Visualization\n",
    "\n",
    "Visualize embeddings using t-SNE to see the clustering of identities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "085779fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_embeddings(test_dict, num_persons=30):\n",
    "    \"\"\"\n",
    "    Create t-SNE visualization of embeddings\n",
    "    \n",
    "    Args:\n",
    "        test_dict: Dictionary with person IDs as keys and lists of embeddings as values\n",
    "        num_persons: Number of persons to visualize\n",
    "    \"\"\"\n",
    "    # Collect embeddings and labels\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    person_ids = []\n",
    "    \n",
    "    # Take first num_persons persons\n",
    "    for i, (person_id, person_embeddings) in enumerate(test_dict.items()):\n",
    "        if i >= num_persons:\n",
    "            break\n",
    "        embeddings.extend(person_embeddings)\n",
    "        labels.extend([i] * len(person_embeddings))\n",
    "        person_ids.append(person_id)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    embeddings_np = torch.stack(embeddings).numpy()\n",
    "    \n",
    "    # Apply t-SNE\n",
    "    print(\"Computing t-SNE projection...\")\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(embeddings)-1))\n",
    "    embeddings_tsne = tsne.fit_transform(embeddings_np)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Use a color cycle\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, num_persons))\n",
    "    \n",
    "    for i in range(num_persons):\n",
    "        indices = [j for j, label in enumerate(labels) if label == i]\n",
    "        plt.scatter(\n",
    "            embeddings_tsne[indices, 0], \n",
    "            embeddings_tsne[indices, 1], \n",
    "            color=colors[i],\n",
    "            label=f\"Person {person_ids[i]}\", \n",
    "            s=30,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.title(\"t-SNE Visualization of Biometric Embeddings\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f\"{params['output_dir']}/embeddings_tsne.png\", dpi=300)\n",
    "    \n",
    "    # Create a version without the legend for clarity\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    for i in range(num_persons):\n",
    "        indices = [j for j, label in enumerate(labels) if label == i]\n",
    "        plt.scatter(\n",
    "            embeddings_tsne[indices, 0], \n",
    "            embeddings_tsne[indices, 1], \n",
    "            color=colors[i],\n",
    "            s=30,\n",
    "            alpha=0.7\n",
    "        )\n",
    "    \n",
    "    plt.title(\"t-SNE Visualization of Biometric Embeddings\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.savefig(f\"{params['output_dir']}/embeddings_tsne_no_legend.png\", dpi=300)\n",
    "    \n",
    "    print(\"t-SNE visualization saved\")\n",
    "    return embeddings_tsne, labels, person_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e260a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embeddings with t-SNE\n",
    "print(\"Creating t-SNE visualization...\")\n",
    "embeddings_tsne, labels, person_ids = visualize_embeddings(test_dict, num_persons=params['visualize_persons'])\n",
    "plt.show()\n",
    "\n",
    "print(\"Testing completed! All results have been saved to:\", params['output_dir'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d3b2a6",
   "metadata": {},
   "source": [
    "## Summary of Results\n",
    "\n",
    "Summarize all evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6480b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary of all results\n",
    "summary = {\n",
    "    'model': params['model_path'],\n",
    "    'rank1': {\n",
    "        'best': max(r['rank1_rate'] for r in results),\n",
    "        'gallery_size': results[np.argmax([r['rank1_rate'] for r in results])]['gallery_size']\n",
    "    },\n",
    "    'verification': {\n",
    "        'roc_auc': verification_metrics['roc_auc'],\n",
    "        'eer': verification_metrics['eer']\n",
    "    },\n",
    "    'modalities': {\n",
    "        m: modality_results[m]['rank1_rate'] for m in ['periocular', 'forehead', 'iris', 'fusion']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n===== SUMMARY OF RESULTS =====\")\n",
    "print(f\"Model: {summary['model']}\")\n",
    "print(f\"Best Rank-1 Recognition Rate: {summary['rank1']['best']*100:.2f}% (Gallery Size: {summary['rank1']['gallery_size']})\")\n",
    "print(f\"ROC AUC: {summary['verification']['roc_auc']:.4f}\")\n",
    "print(f\"Equal Error Rate (EER): {summary['verification']['eer']*100:.2f}%\")\n",
    "print(\"\\nModality Performance (Rank-1):\")\n",
    "for modality, rate in summary['modalities'].items():\n",
    "    print(f\"  {modality.capitalize()}: {rate*100:.2f}%\")\n",
    "print(\"===============================\")\n",
    "\n",
    "# Save summary to file\n",
    "with open(f\"{params['output_dir']}/summary.txt\", 'w') as f:\n",
    "    f.write(\"===== SUMMARY OF RESULTS =====\\n\")\n",
    "    f.write(f\"Model: {summary['model']}\\n\")\n",
    "    f.write(f\"Best Rank-1 Recognition Rate: {summary['rank1']['best']*100:.2f}% (Gallery Size: {summary['rank1']['gallery_size']})\\n\")\n",
    "    f.write(f\"ROC AUC: {summary['verification']['roc_auc']:.4f}\\n\")\n",
    "    f.write(f\"Equal Error Rate (EER): {summary['verification']['eer']*100:.2f}%\\n\\n\")\n",
    "    f.write(\"Modality Performance (Rank-1):\\n\")\n",
    "    for modality, rate in summary['modalities'].items():\n",
    "        f.write(f\"  {modality.capitalize()}: {rate*100:.2f}%\\n\")\n",
    "    f.write(\"===============================\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
